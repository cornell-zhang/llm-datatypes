## Copyright (c) 2021 Intel Corporation
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##    http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
##
#

-
  version:
    name: '1.6.0'
  weight_only_integer: &cap_weight_only {
    'MatMul': &cap_weight_only_matmul {
        'weight': {
                    'dtype': ['int'], # no need to care uint
                    'bits': [4, 3, 8], # [1-8]
                    'group_size': [32, -1, 1, 16, 64, 128, 256, 512, 1024], # [1-inf]
                    'scheme': ['sym', 'asym'], # sym, no ZP
                    'algorithm': ['RTN', 'AWQ', 'GPTQ']
        },
        'activation': {
                    'dtype': ['fp32']
        }
    },
  }
  int8: &ref_1_6 {
    'static': &ref_1_6_static {
      'Conv': {
        'weight':   &int8_sym_perchanneltensor_minmax {
                    'dtype': ['int8'],
                    'scheme': ['sym'],
                    'granularity': ['per_channel', 'per_tensor'],
                    'algorithm': ['minmax']
                    },
        'activation': &uint8_asym_pertensor_minmax {
                    'dtype': ['uint8'],
                    'scheme': ['asym'],
                    'granularity': ['per_tensor'],
                    'algorithm': ['minmax', 'kl', 'percentile']
                    },
        'mode': ['QDQ', 'QLinear']
      },
      'FusedConv': {
        'weight':   *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   &uint8_asym_perchanneltensor_minmax {
                    'dtype': ['uint8'],
                    'scheme': ['asym'],
                    'granularity': ['per_channel', 'per_tensor'],
                    'algorithm': ['minmax']
                    },
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   &int8_sym_pertensor_minmax {
                    'dtype': ['int8'],
                    'scheme': ['sym'],
                    'granularity': ['per_tensor'],
                    'algorithm': ['minmax']
                    },
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': &default_static_qlinear_qdq {
        'weight':   *int8_sym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Mul': &default_static_qlinear {
        'weight':   *int8_sym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QLinear']
      },
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'EmbedLayerNormalization': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
    },
    'dynamic': &ref_1_6_dynamic {
      'Conv': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'FusedConv': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'MatMul': &default_dynamic {
        'weight': *int8_sym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'Gather': *default_dynamic,
      'Attention': *default_dynamic,
      'EmbedLayerNormalization': *default_dynamic,
      'LSTM': *default_dynamic,
    }
  }
  fp16: &common_fp16 ['Concat', 'Gather', 'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze',
    'EmbedLayerNormalization', 'Attention', 'Split', 'Sigmoid', 'Relu', 'Mul', 'Pad', 'MaxPool',
    'MatMul', 'LeakyRelu',  'GlobalAveragePool', 'Gemm', 'Conv', 'AveragePool', 'Add', 'Clip',
    'BatchNormalization', 'Softmax', 'Sum', 'Abs', 'BiasGelu', 'Exp', 'FastGelu',
    'Gelu', 'Log', 'Round', 'Sigmoid', 'Sqrt', 'Tanh', 'Sub', 'Mul', 'Div', 'Pow',
    'ReduceMean', 'Equal', 'FusedMatMul', 'Greater', 'GreaterOrEqual', 'Less', 'LessOrEqual',
    'ReduceL1', 'ReduceL2', 'ReduceLogSum', 'ReduceLogSumExp', 'ReduceMax', 'ReduceProd',
    'ReduceSum', 'ReduceSumSquare', 'LayerNormalization', 'Concat']
  bf16: &common_bf16 ['Concat', 'Gather', 'Reshape', 'Squeeze', 'Transpose', 'Unsqueeze',
    'Split', 'Sigmoid', 'Relu', 'Mul', 'MatMul', 'Gemm', 'Add']
  recipes: &default_optimization
    graph_optimization:   # from onnxruntime graph_optimization_level
      level: ['DISABLE_ALL', 'ENABLE_BASIC', 'ENABLE_EXTENDED', 'ENABLE_ALL']

-
  version:
    name: '1.7.0'
  weight_only_integer: *cap_weight_only
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, #'QDQ': *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': *default_static_qlinear_qdq,
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'EmbedLayerNormalization': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
    },
    'dynamic': *ref_1_6_dynamic
  }
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.8.0'
  weight_only_integer: *cap_weight_only
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':  *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'EmbedLayerNormalization': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': {
      'Conv': {
        'weight': *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'FusedConv': {
        'weight': *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'MatMul': {
        'weight': *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'Gather': *default_dynamic,
      'Attention': *default_dynamic,
      'EmbedLayerNormalization': *default_dynamic,
      'LSTM': *default_dynamic,
    }
  }
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.9.0'
  weight_only_integer: *cap_weight_only
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': &ref_1_9_dynamic {
      'Conv': {
        'weight': *uint8_asym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'FusedConv': {
        'weight': *uint8_asym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'MatMul': {
        'weight': *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax,
        'activation': *uint8_asym_pertensor_minmax
      },
      'Gather': *default_dynamic,
      'Attention': *default_dynamic,
      'LSTM': *default_dynamic,
    }
  }
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.10.0'
  weight_only_integer: *cap_weight_only
  int8: {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'Resize': *default_static_qlinear_qdq,
    },
    'dynamic': *ref_1_9_dynamic
  }
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.11.0'
  weight_only_integer: *cap_weight_only
  int8: &ref_1_11 {
    'static': {
      'FusedConv': {
        'weight': *int8_sym_perchanneltensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Conv': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gather': {
        'weight':   *uint8_asym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'MatMul': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Gemm': {
        'weight':   *int8_sym_perchanneltensor_minmax,
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'EmbedLayerNormalization': {
        'weight': *uint8_asym_pertensor_minmax, # QDQ: *int8_sym_pertensor_minmax
        'activation': *uint8_asym_pertensor_minmax,
        'mode': ['QDQ', 'QLinear']
      },
      'Attention': *default_static_qlinear_qdq,
      'Mul': *default_static_qlinear,
      'Relu': *default_static_qlinear_qdq,
      'Clip': *default_static_qlinear_qdq,
      'LeakyRelu': *default_static_qlinear_qdq,
      'Sigmoid': *default_static_qlinear_qdq,
      'MaxPool': *default_static_qlinear_qdq,
      'GlobalAveragePool': *default_static_qlinear_qdq,
      'Pad': *default_static_qlinear_qdq,
      'Split': *default_static_qlinear_qdq,
      'Add': *default_static_qlinear,
      'Squeeze': *default_static_qlinear_qdq,
      'Reshape': *default_static_qlinear_qdq,
      'Concat': *default_static_qlinear_qdq,
      'AveragePool': *default_static_qlinear_qdq,
      'Unsqueeze': *default_static_qlinear_qdq,
      'Transpose': *default_static_qlinear_qdq,
      'ArgMax': *default_static_qlinear,
      'Resize': *default_static_qlinear_qdq,
      'Abs': *default_static_qlinear_qdq,
      'Shrink': *default_static_qlinear_qdq,
      'Sign': *default_static_qlinear_qdq,
      'Flatten': *default_static_qlinear_qdq,
      'Expand': *default_static_qlinear_qdq,
      'Slice': *default_static_qlinear_qdq,
      'Mod': *default_static_qlinear_qdq,
      'ReduceMax': *default_static_qlinear_qdq,
      'ReduceMin': *default_static_qlinear_qdq,
    },
    'dynamic': *ref_1_9_dynamic
  }
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization

-
  version:
    name: '1.12.0'
  weight_only_integer: *cap_weight_only
  int8: *ref_1_11
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization

-
  version:
    name: 'default'
  weight_only_integer: *cap_weight_only
  int8: *ref_1_6
  fp16: *common_fp16
  bf16: *common_bf16
  recipes:
    <<: *default_optimization
