#
# Copyright (c) 2021 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
-
  version:
    name: 'default'
    # (MXNet-specific):
    # 'int8' for this version specifies quantization in general (both int8 and uint8), as
    # quantization dtype is selected automatically for each quantized node according to the
    # calibration results: min_value < 0 => int8; min_value >= 0 => uint8.
    # 'int8' here means 'auto' in MXNet

  precisions: &common_precisions
    names: int8, fp32
    valid_mixed_precisions: []

  ops: &common_ops
    int8: [ 'quantize_output',
            'Flatten', 'Pooling', 'Convolution', 'FullyConnected',
            '_sg_mkldnn_conv', '_sg_mkldnn_fully_connected',
            '_sg_mkldnn_selfatt_qk', '_sg_mkldnn_selfatt_valatt',
            '_sg_onednn_conv', '_sg_onednn_fully_connected',
            '_sg_onednn_selfatt_qk', '_sg_onednn_selfatt_valatt' ]
    fp32: ['*']

  capabilities: &common_capabilities
    int8: {
        'default': &capability_default [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']},
            'weight': {
                'dtype': ['int8'],
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax']}
            },
            {'activation': {
                'dtype': 'fp32'},
            'weight': {
                'dtype': 'fp32'}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']},
            'weight': {
                'dtype': ['int8'],
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax']}
            }
        ],
        'quantize_output': &capability_quantize [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            }
        ],
        'Flatten': &capability_flatten [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': 'fp32'}
            }
        ],
        'Pooling': &capability_pooling [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': 'fp32'}
            }
        ],
        'Convolution': &capability_conv [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']},
            'weight': {
                'dtype': ['int8'],
                'scheme': ['sym'],
                'granularity': ['per_channel'],
                'algorithm': ['minmax']}
            },
            {'activation': {
                'dtype': 'fp32'},
            'weight': {
                'dtype': 'fp32'}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']},
            'weight': {
                'dtype': ['int8'],
                'scheme': ['sym'],
                'granularity': ['per_channel'],
                'algorithm': ['minmax']}
            }
        ],
        'FullyConnected': &capability_fc [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']},
            'weight': {
                'dtype': ['int8'],
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax']}
            },
            {'activation': {
                'dtype': 'fp32'},
            'weight': {
                'dtype': 'fp32'}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']},
            'weight': {
                'dtype': ['int8'],
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax']}
            }
        ],
        '_sg_mkldnn_conv': *capability_conv,
        '_sg_mkldnn_fully_connected': *capability_fc,
        '_sg_mkldnn_selfatt_qk': &capability_mkldnn_qk [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': 'fp32'}
            }
        ],
        '_sg_mkldnn_selfatt_valatt': &capability_mkldnn_valatt [
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'static',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': ['int8'],
                'quant_mode': 'dynamic',
                'scheme': ['sym'],
                'granularity': ['per_tensor'],
                'algorithm': ['minmax', 'kl']}
            },
            {'activation': {
                'dtype': 'fp32'}
            }
        ],
        '_sg_onednn_conv': *capability_conv,
        '_sg_onednn_fully_connected': *capability_fc,
        '_sg_onednn_selfatt_qk': *capability_mkldnn_qk,
        '_sg_onednn_selfatt_valatt':  *capability_mkldnn_valatt
      }

  patterns: &common_patterns
  # (MXNet-specific):
  # fusion patterns are hardcoded in the framework
    fp32: [
        # conv + bn
        # conv + act + sum
        # conv + add
        # conv + bn + act
        # conv + bn + add + act
        # conv + bn + sum + act
        # fc + relu
    ]
    int8: []

-
  version:
    name: '1.6.0'

  precisions:
    <<: *common_precisions

  ops:
    <<: *common_ops

  capabilities:
    << : *common_capabilities

  patterns:
    << : *common_patterns

-
  version:
    name: '1.7.0'

  precisions:
    <<: *common_precisions

  ops:
    <<: *common_ops

  capabilities:
    << : *common_capabilities

  patterns:
    << : *common_patterns

-
  version:
    name: '1.8.0'

  precisions:
    <<: *common_precisions

  ops:
    <<: *common_ops

  capabilities:
    << : *common_capabilities

  patterns:
    << : *common_patterns

-
  version:
    name: '1.9.0'

  precisions:
    <<: *common_precisions

  ops:
    <<: *common_ops

  capabilities:
    << : *common_capabilities

  patterns:
    << : *common_patterns

-
  version:
    name: '2.0.0'

  precisions:
    names: int8, fp32, bf16
    valid_mixed_precisions: []

  ops:
    <<: *common_ops
    bf16: [ 'Convolution', 'Deconvolution', 'FullyConnected',
            '_sg_mkldnn_conv', '_sg_mkldnn_fully_connected',
            '_sg_mkldnn_selfatt_qk', '_sg_mkldnn_selfatt_valatt',
            '_sg_onednn_conv', '_sg_onednn_fully_connected',
            '_sg_onednn_selfatt_qk', '_sg_onednn_selfatt_valatt' ]

  capabilities:
    << : *common_capabilities
    bf16: {
        'Convolution': &bf16_capability_conv {
            'activation': {'dtype': ['bf16', 'fp32']},
            'weight':     {'dtype': ['bf16', 'fp32']},
        },
        'FullyConnected': &bf16_capability_fc {
            'activation': {'dtype': ['bf16', 'fp32']},
            'weight':     {'dtype': ['bf16', 'fp32']},
        },
        '_sg_mkldnn_conv': *bf16_capability_conv,
        '_sg_mkldnn_fully_connected': *bf16_capability_fc,
        '_sg_mkldnn_selfatt_qk': &bf16_capability_mkldnn_qk {
            'activation': {'dtype': ['bf16', 'fp32']},
        },
        '_sg_mkldnn_selfatt_valatt': &bf16_capability_mkldnn_valatt {
            'activation': {'dtype': ['bf16', 'fp32']},
        },
        '_sg_onednn_conv': *bf16_capability_conv,
        '_sg_onednn_fully_connected': *bf16_capability_fc,
        '_sg_onednn_selfatt_qk': *bf16_capability_mkldnn_qk,
        '_sg_onednn_selfatt_valatt':  *bf16_capability_mkldnn_valatt
      }


  patterns:
    << : *common_patterns
